---
layout: page
title: Probability
---

### What is multinomial distribution?
The distribution that assigns probabilities to a number of discrete choices is called the multinomial distribution.

### Draw a sample dice in pytorch
```
import torch
from torch.distributions import multinomial

fair_probs = torch.ones([6]) / 6
multinomial.Multinomial(1, fair_probs).sample()
```

### What is central limit theorem?
The central limit theorem states that if you have a population with mean Î¼ and standard deviation Ïƒ and take sufficiently large random samples from the population with replacement , then the distribution of the sample means will be approximately normally distributed with the population mean and standard deviation.

### Write some pseudo code to demonstrate central limit theorem
```
counts = multinomial.Multinomial(10, fair_probs).sample((500,))
cum_counts = counts.cumsum(dim=0)
estimates = cum_counts / cum_counts.sum(dim=1, keepdims=True)

set_figsize((6, 4.5))
for i in range(6):
    plt.plot(estimates[:, i].numpy(),
                 label=("P(die=" + str(i + 1) + ")"))
plt.axhline(y=0.167, color='black', linestyle='dashed')
plt.gca().set_xlabel('Groups of experiments')
plt.gca().set_ylabel('Estimated probability')
plt.legend();
```

### What are the axioms of probability?
1. For any event A, its probability is never negative, i.e.,  ğ‘ƒ(A)â‰¥0 ;
1. Probability of the entire sample space is  1 , i.e.,  ğ‘ƒ(S)=1 ;
1. For any countable sequence of events  A1,A2,â€¦  that are mutually exclusive ( Aiâˆ©Aj=âˆ…  for all  ğ‘–â‰ ğ‘— ), the probability that any happens is equal to the sum of their individual probabilities

### What is a random variable?
A random variable can be pretty much any quantity and is not deterministic. It could take one value among a set of possibilities in a random experiment. Note that there is a subtle difference between discrete random variables, like the sides of a die, and continuous ones, like the weight and the height of a person. There is little point in asking whether two people have exactly the same height.

### What is joint probability?
Given any values  ğ‘  and  ğ‘ , the joint probability lets us answer, what is the probability that  ğ´=ğ‘  and  ğµ=ğ‘  simultaneously.

### What is conditional probability?
Note that for any values  ğ‘  and  ğ‘ ,  ğ‘ƒ(ğ´=ğ‘,ğµ=ğ‘)â‰¤ğ‘ƒ(ğ´=ğ‘) . This has to be the case, since for  ğ´=ğ‘  and  ğµ=ğ‘  to happen,  ğ´=ğ‘  has to happen and  ğµ=ğ‘  also has to happen (and vice versa). Thus,  ğ´=ğ‘  and  ğµ=ğ‘  cannot be more likely than  ğ´=ğ‘  or  ğµ=ğ‘  individually. This brings us to an interesting ratio:  0â‰¤ğ‘ƒ(ğ´=ğ‘,ğµ=ğ‘)/ğ‘ƒ(ğ´=ğ‘)â‰¤1 . We call this ratio a conditional probability and denote it by  ğ‘ƒ(ğµ=ğ‘âˆ£ğ´=ğ‘) : it is the probability of  ğµ=ğ‘ , provided that  ğ´=ğ‘  has occurred.

### What is Bayes theorem?
ğ‘ƒ(ğ´âˆ£ğµ) = ğ‘ƒ(ğµâˆ£ğ´)ğ‘ƒ(ğ´)/ğ‘ƒ(ğµ)

### What is Marginalization?
It is the operation of determining  ğ‘ƒ(ğµ)  from  ğ‘ƒ(ğ´,ğµ) . We can see that the probability of  ğµ  amounts to accounting for all possible choices of  ğ´  and aggregating the joint probabilities over all of them: ğ‘ƒ(ğµ)=âˆ‘ğ‘ƒ(ğ´,ğµ).

### What is Independence?
Two random variables  ğ´  and  ğµ  being independent means that the occurrence of one event of  ğ´  does not reveal any information about the occurrence of an event of  ğµ . In this case  ğ‘ƒ(ğµâˆ£ğ´)=ğ‘ƒ(ğµ) . Statisticians typically express this as  ğ´âŠ¥ğµ . From Bayesâ€™ theorem, it follows immediately that also  ğ‘ƒ(ğ´âˆ£ğµ)=ğ‘ƒ(ğ´) . In all the other cases we call  ğ´  and  ğµ  dependent.

Likewise, two random variables  ğ´  and  ğµ  are conditionally independent given another random variable  ğ¶  if and only if  ğ‘ƒ(ğ´,ğµâˆ£ğ¶)=ğ‘ƒ(ğ´âˆ£ğ¶)ğ‘ƒ(ğµâˆ£ğ¶) . This is expressed as  ğ´âŠ¥ğµâˆ£ğ¶ .

### What is Expectation?
The expectation (or average) of the random variable  ğ‘‹  is denoted as: ğ¸[ğ‘‹]=âˆ‘ğ‘¥ğ‘¥ğ‘ƒ(ğ‘‹=ğ‘¥).
 
When the input of a function  ğ‘“(ğ‘¥)  is a random variable drawn from the distribution  ğ‘ƒ  with different values  ğ‘¥ , the expectation of  ğ‘“(ğ‘¥)  is computed as: ğ¸ğ‘¥âˆ¼ğ‘ƒ[ğ‘“(ğ‘¥)]=âˆ‘ğ‘¥ğ‘“(ğ‘¥)ğ‘ƒ(ğ‘¥).

### What are Variance and Standard Deviation?
In many cases we want to measure by how much the random variable  ğ‘‹  deviates from its expectation. This can be quantified by the variance: Var[ğ‘‹]=ğ¸[(ğ‘‹âˆ’ğ¸[ğ‘‹])**2]=ğ¸[ğ‘‹**2]âˆ’ğ¸[ğ‘‹]**2.
 
Its square root is called the standard deviation. The variance of a function of a random variable measures by how much the function deviates from the expectation of the function, as different values  ğ‘¥  of the random variable are sampled from its distribution: Var[ğ‘“(ğ‘¥)]=ğ¸[(ğ‘“(ğ‘¥)âˆ’ğ¸[ğ‘“(ğ‘¥)])**2].

